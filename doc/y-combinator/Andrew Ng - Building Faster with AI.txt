Andrew Ng: Really great to see all of you. What I want to do today, since this is built as startup school, is share with you some lessons I've learned about building startups at AI Fund. AI Fund is a venture studio, and we build an average of about one startup per month. And because we co-founded startups, we're in there writing codes, talking to customers, designing our features, determining pricing. And so we've done a lot of reps of not just watching others build startups, but actually being in the weeds building startups with entrepreneurs. And what I want to do today is Here I've used some of the lessons I've learned building startups, especially around this changing AI technology and what it enables.
Andrew Ng: And it will be focused on the theme of speed. So it turns out that for those of you that want to build a startup, I think a strong predictor for startups' odds of success is execution speed. And I should have a lot of respect for the entrepreneurs and executives that can just do things really quickly. And new AI technology is enabling startups to go much faster. So what I hope to do is share with you some of those best practices, which are frankly changing every two to three months though, to let you get that speed that hopefully lets you have a higher odds of success.
Andrew Ng: Before diving into speed, a lot of people ask me, hey Andrew, where are the opportunities for startups? So this is what I think of as an AI stack. where at the lowest level are the semiconductor companies, then the clouds are hyperscalers built on top of that, a lot of the AI foundation model companies built on top of that, and even though a lot of the PR excitement and hype has been on these technology layers, it turns out that almost by definition, the biggest opportunities have to be at the application layer, Because we actually need the applications to generate even more revenue so that they can afford to pay the foundation, cloud, and semiconductor technology layers.
Andrew Ng: So for whatever reason, media and social media tends not to talk about the application layer as much. But for those of you thinking of building startups, almost by definition, the biggest opportunities have to be there. Although, of course, the opportunity is at all layers of the stack. One of the things that's changed a lot over the last year, and in terms of AI tech trends, if you ask me, what's the most important tech trend in AI, I would say is the rise of agentic AI. And about a year and a half ago, when I started to go around and give talks to try to convince people that AI agents might be a thing, I did not realize that around last summer, a bunch of marketers would get a hold of this term and use it as a sticker and slap it on everything in sight, which made it almost lose some of its meaning.
Andrew Ng: But I want to share with you, from a technical perspective, why I think agentic AI is exciting and important, and also opens up a lot more startup opportunities. So it turns out that the way a lot of us use LLMs is to prompt it, to have it during an output. And the way we have an LLM output something is as if you're going to a human, or in this case an AI, and asking it to please type on an essay for you by writing from the first word to the last word all in one go without ever using backspace.
Andrew Ng: And humans, we don't do our best writing being forced to type in this linear order. And it turns out neither does AI, but despite the difficulty of being forced to write in this linear way, our LMS do surprisingly well. With agentic workflows, we can go to AI system and ask it to please first write an essay outline, then do some web research if it needs to and pitch some web pages to put in their own context, then write the first draft, then read the first draft and critique it and revise it and so on. And so we end up with this iterative workflow where your model does some thinking and some research, does some revision, goes back to do more thinking.
Andrew Ng: And by going around this loop many times, it is slower, but it delivers a much better work product. So for a lot of the projects that AI Fund has worked on, everything from pulling out complex compliance documents to medical diagnosis to reasoning about complex legal documents, we found that these agentic workflows are really a huge difference between working versus not working. But a lot of the work that needs to be done, a lot of valuable businesses to be built still, will be taking workflows, existing only workflows, and figuring out how to implement them into these types of agentic workflows.
Andrew Ng: So just to update the picture for the AI stack, what has emerged over the last year is a new agentic orchestration layer that helps application builders orchestrate or coordinate a lot of calls to the technology layers underneath. And the good news is the orchestration layer has made it even easier to build applications. But I think the basic conclusion is that the application layer has to be the most valuable layer of the stack. still holds true. With a bias or focus on the application layer, let me now dive into some of the best practices I've learned about how startups can move faster.
Andrew Ng: It turns out that at AI Fund, we only focus on working on concrete ideas. So to me, a concrete idea, a concrete product idea, is one that's specified in enough detail that an engineer can go and build it. So for example, if you say, let's use AI to optimize healthcare assets, that's actually not a concrete idea, it's too vague. If you tell me to write software to use AI to optimize healthcare assets, different engineers would do totally different things. And because it's not concrete, you can't build it quickly and you don't have speed. In contrast, if you had a concrete idea, let's write software to let hospitals, let patients book MRI machine slots online to optimize usage.
Andrew Ng: I don't know if this is a good or a bad concrete idea. It's actually business already doing this. But it is concrete, and that means engineers can build it quickly. If it's a good idea, you find out. If it's not a good idea, you will find out. But having concrete ideas buys you speed. Or someone would just say, let's use AI for email personal productivity. Too many interpretations of that. That's not concrete. But if someone says, could you build an app? Gmail integrates automation. Let's use the right prompts. Filter entire emails. That is concrete. I could go build that this afternoon.
Andrew Ng: So concreteness buys you speed. And the deceptive thing for a lot of entrepreneurs is the vague ideas tend to get a lot of kudos. If you go and tell all your friends, we should use AI to optimize the use of healthcare assets, everyone will say that's a great idea. But it's actually not a great idea, at least in the sense of being something you can build. It turns out when you're vague, you're almost always right. But when you're concrete, you may be right or wrong. Either way, it's fine. We can discover that much more fast, which is what's important for a startup.
Andrew Ng: In terms of executing concrete ideas, I find that in AI fun, I ask my team to focus on concrete ideas because a concrete idea gives clear direction and the team can run really fast to build it and either validate it, prove it out, or falsify it and conclude it doesn't work. Either way is fine, so let's do that quickly. And it turns out that finding good concrete ideas usually requires someone, could be you, could be a subject matter expert, thinking about a problem for a long time. So for example, actually before starting Coursera, I spent years thinking about online education, talking to users, holding my own intuitions about what would make a good ad tech platform.
Andrew Ng: And then after that long process, I think YC sometimes calls it wondering the idea maze. But after thinking about it for a long time, you find that the guts of people that have thought about this for a long time can be very good about rapidly making decisions. As in, after you've thought about this, talk to customers and so on for a long time, if you ask this expert, should I build this feature or that feature? The gut, which is an instantaneous decision, can be actually a surprisingly good proxy, can be a surprisingly good mechanism for making decisions.
Andrew Ng: And I know I work on AI, you might think I'll say, oh, we need data. And of course, I love data. But it turns out getting data for a lot of startups is actually a slow mechanism for making decisions. And a subject matter expert with a good gut is often a much better mechanism for making a speedy decision. And then one other thing, for many successful startups, at any moment in time, you're pursuing one very clear hypothesis that you're building out and trying to sell, trying to validate or falsify. A startup doesn't have resources to hedge and try 10 things at the same time.
Andrew Ng: So pick one, go for it. And if data tells you to lose faith in that idea, that's actually totally fine. Just pivot on the dime to pursue a totally different concrete idea. So that's what often feels like an AI fund. We're pursuing one thing doggedly with determination until the world tells us we were wrong, then change and pursue a totally different thing with equal determination and equal doggedness. And one of the patterns I've seen, if every piece of new data calls you to the pivot, it probably means you're starting off from too weak a base of knowledge, right?
Andrew Ng: If every time you talk to a customer, you totally change your mind, probably means you don't know enough about that sector yet to have a really high quality concrete idea. And finding someone that's thought about a subject for longer may get you onto a better path. In order to go faster, the other thing I often think about is the build feedback group, which is rapidly changing when it comes to how we build with AI coding assistance. So when you're building a lot of applications, one of the biggest risks is customer acceptance. A lot of startups struggle, not because we can't build whatever we want to build, but because we build something and it turns out nobody cares.
Andrew Ng: And so for a lot of the way I build startups, especially applications, less so deep tech, less so technology startups but definitely application startups is we often build software so this is an engineering task and then we will get feedback from users and this is a product management task and then we'll go back you know then based on the user feedback we'll tweak our views on what to build go back to write more software and we go around this loop many many times iterate toward product market fit and it turns out that With AI coding assistants, which Andre talked about as well, rapid engineering is becoming possible in a way that just was not possible.
Andrew Ng: It is becoming much more feasible. So the speed of engineering is going up rapidly, and the cost of engineering is also going down rapidly. This changes the mechanisms by which we drive startups around this loop. When I think about the software that I do, I may be put into two major buckets. Sometimes I've built quick and dirty prototypes to test an idea. Say, build a new customer service chatbot, let's build an AI to process legal documents or whatever. Build a quick and dirty prototype to see if we think it works. The other type of software where I do is maintain production software, maintain legacy software, but these massive production-ready code bases.
Andrew Ng: Depending on which analyst report you trust, it's been hard to find very rigorous data on this. You know, when writing production quality code, maybe we're 30 to 50% faster with AR systems. Hard to find a rigorous number. Maybe. Be as plausible to be. But in terms of building quick and dirty prototypes, we're not 50% faster. I think we're easily 10 times faster, maybe much more than 10 times faster. And there are a few reasons for this. When you're building standalone prototypes, there's less integration with legacy software infrastructure, legacy data needed. Also, the requirements for reliability, even scalability, even security are much lower.
Andrew Ng: And I know I'm not supposed to tell people to write insecure code, right? It feels like the wrong thing to say. But I routinely go to my teams and they go ahead and write insecure code. Because if this software is only going to run on your laptop, and you don't plan to maliciously hack your own laptop, it's fine to have insecure code, right? But of course, after it seems to be working, please do make it secure before you ship it to someone else. And you know, like a leaking PII, leaking sensitive data, that is very damaging.
Andrew Ng: So before you ship it, make it secure and scalable, but they were just testing it, it's fine. And so I find increasingly Stata will systematically pursue innovations by building 20 prototypes to see what works. Because I know that there's some ants in AI, a lot of proof of concepts don't make it into production, but I think by driving the cost of a proof of concept low enough, it's actually fine if lots of proof of concepts don't see the light of day. And I know that the mantra, move fast and break things, got a bad rep, because it broke things.
Andrew Ng: And some teams took away from this that you should not move fast, but I think that's a mistake. I tend to tell my teams to move fast and be responsible. And I think they actually lost the ways to move really quickly while still being responsible. And in terms of the AI assistance coding landscape, I think was it three, four years ago, code auto-complete, right, popularized by GitHub co-pilot. And then there was a cursor windsurf generation of AI enabled IDEs, really great use windsurf and cursor. quite a lot. And then starting, I don't know, six, seven months ago, there started to be this new generation of highly agentic coding assistants, including that she's using O3 a lot for coding.
Andrew Ng: Cloud Code is fantastic. Since Quad 4 release, it's become, and I'll speak again in three months, I may use something different, but the tools are evolving. really rapidly, but I think cloud code codecs is a new generation of highly agentic coding assistants that is making developer productivity keep on growing. And the interesting thing is if you're even half a generation or one generation behind, it actually makes a big difference compared to if you're on top of the latest tools. And I find my team is taking really different approaches to software engineering now compared to even three or six months ago.
Andrew Ng: One surprising thing is we're used to thinking of code as this really valuable artifact because it's so hard to create. But because the cost of software engineers going down, code is much less of a valuable artifact as it used to. Some on teams where we've completely rebuilt the code base three times the last month, right? Because it's not that hard anymore to just completely rebuild the code base, pick a new data schema, it's fine because the cost of doing that has plummeted. Some of you may have heard of Jeff Bezos' terminology of a two-way door versus a one-way door.
Andrew Ng: A two-way door is a decision they can make. If you change your mind, come back out, you know, reverse it relatively cheaply. Whereas a one-way door is you make a decision and you change your mind, it's very costly, very difficult to reverse. So choosing the software architecture of your tech stack used to be a one-way door. Once you built on top of a certain tech stack, you know, set a database schema, really hard to change it. So that used to be a one-way door. I don't want to say it's totally a two-way door, but I find that my team will more often build on a certain tech stack a week later, change your mind, let's throw the code base away and redo it from scratch in the new tech stack.
Andrew Ng: I don't want to over-hype it. We don't do that all the time. There are still costs to redoing that. But I find my team's often rethinking what is a one-way door and what's now a two-way door because the cost of software engineering is so much lower now. And maybe going a little bit beyond software engineering, I feel like it's actually a good time to empower everyone to build AI. Over the last year, a bunch of people have advised others not to learn to code on the grounds that AI will automate it. I think we'll look back on this as some of the worst career advice ever given.
Andrew Ng: Because as better tools make software engineering easier, more people should do it, not fewer. So when, many decades ago, the world moved from punch cards to keyboard and terminal, that made coding easier. When we moved from assemblies, high-level languages like COBOL, there were actually people arguing back then that now we have COBOL, we don't need programmers anymore. People actually wrote papers to that effect. But of course, that was wrong, and programming languages made it easier to code, and more people learned to code. Text messages to IDEs, IDEs to AI coding assistants, and as coding becomes easier, more people should learn to code.
Andrew Ng: I have a controversial opinion, which is I think it's time for everyone of every job role to learn to code. And in fact, on my team, my CFO, my head of talent, my recruiters, my front desk person, all of them know how to code. And I actually see all of them performing better at all of their job functions because they can code. And I think I'm probably a little bit ahead of her. I'm afraid most businesses are not there yet. But in the future, I think we'll empower everyone to code. A lot of people can be more productive.
Andrew Ng: I want to share with you one lesson I learned as well on why we should have people learn to do this, which is when I was teaching generative AI for everyone on Coursera, we needed to generate background art like this using mid-journey, and one of my team members knew art history, and so he could prompt mid-journey with the genre, the palette, the artistic inspiration, had a very good control over the images he generated. So we ended up using all of Tommy's generated images. Whereas in contrast, I don't know art history. And so when I prompt the image generation, I could write, please make pretty pictures of robots for me, right?
Andrew Ng: And I could never have the control that my collaborates are good. And so I couldn't generate as good images as he could. And I think with computers, one of the most important skills of the future is the ability to tell a computer exactly what you want so they will do it for you. And it will be people that have that deeper understanding of computers that will be able to command the computer to get the outcome you want. And learning to code, not that you need to write the code yourself, steer AI to code for you, seems like it would remain the best way to do that for a long time.
Andrew Ng: With software engineering becoming much faster, the other interesting dynamic I'm seeing is that the product management work, getting user feedback, deciding what features to build, that is increasingly the bottleneck. And so I'm seeing very interesting dynamics in multiple teams. Over the last year, a lot more of my teams have started to complain that they're bottlenecked on product engineering and design because the engineers have gotten so much faster. Some interesting trends I'm seeing, three, four, five years ago, Silicon Valley used to have these slightly suspicious rules of thumb, but nonetheless rules of thumb, will have one PM to four engineers, or one PM to seven engineers.
Andrew Ng: It was just like PM, product manager to engineering ratio, right? We should think of it as a grain of salt, but it was typical as of a one PM to six, seven engineers. And with engineers become much faster. I don't see product management work, designing what to build, becoming faster at the same speed as engineers. I'm seeing this racial shift. So literally yesterday, one of my teams came to me and for the first time, when we're planning a head comfort project, this team proposed to me not to have one PM to four engineers, but to have one PM to 0.5 engineers.
Andrew Ng: So the team actually proposed to me, I still know that it is a good idea. For the first time in my life that I saw, you know, managers proposed to me having twice as many PMs as engineers, which is a very interesting dynamic. I still don't know if this proposal I heard yesterday is a good idea, but I think it's a sign of where the world is going. And I find as PMs they can code, engineers with some product instincts often end up doing better. The other thing that I found important for startup leaders is because engineering is becoming so fast, if you have good tactics for getting rapid feedback to shape your perspective or to build faster, that helps you get faster as well.
Andrew Ng: So I'm going to go through a portfolio of tactics for getting product feedback to keep shaping what you will decide to build. And we're going to go through a list of the faster, maybe less accurate to the slower, more accurate tactics. So the fastest tactic for getting feedback is look at the product yourself and just go by your gut. And if you're a subject matter expert, this is actually surprisingly good if you know what you're doing. A little bit slower is go ask three friends or teammates to get feedback, to play with your product and get feedback.
Andrew Ng: A little bit slower is ask three to ten strangers for feedback. It turns out, when I built products, one of the most important skills I think I learned was how to sit in the coffee shop, how to sit in the air. When I travel, I often sit in the hotel lobby. It turns out, learn to spot places of high foot traffic and very respectfully grab strangers and ask them for feedback on whatever I'm building. This used to be easier when I was less known. When people recognize you, it's a little bit more awkward. I found that I've actually sat with teams in the hotel lobby, very high for traffic, and very respectfully asked strangers, hey, we're building this thing, do you mind taking a look?
Andrew Ng: And I actually learned in the coffee shop that a lot of people working, a lot of people don't want to be working, so we give them an excuse to be distracted, they're very happy to do that too. But I've actually kind of made tons of product decisions in the hotel lobby or the coffee shop with collaborators, just like that. Same prototypes to 100 testers. If you have access to larger groups of users, same prototypes to more users. And these get to be slow and slower tactics. And I know Silicon Valley, we like to talk about A-B testing.
Andrew Ng: Of course, I do a ton of A-B testing, but contrary to what many people think, A-B testing is not one of the slowest tactics. in my menu because it's just slow to ship it, depending on how many users you have. And then the other thing is, as you use anything but the first tactic, some teams will look at the data to make a decision, but the missing piece is when I A-B test something, I don't just use the result of A, B tests to pick product A or product B. My team will often sit down and look carefully at the data to hone our instincts, to speed up, to improve the rate at which we're able to use the first tactic to make high quality decisions.
Andrew Ng: We often sit down and think, gee, I thought this product name will work better than their product name. Clearly, my mental model that uses wrong. So we sit down and think to update our mental model using all of that data to improve the quality of our guts on how to make product decisions faster. That turns out to be really important. All right, so talked about concrete ideas, speed up engineering, speed up product feedback. This is one last thing I want to touch on, which is seeing that understanding AI actually makes you go faster. And here's why.
Andrew Ng: As an AI person, maybe I'm biased to be pro-AI, but I want to share with you why. So it turns out that when it comes to mature technology, like mobile, many people have had smartphones for a long time. We kind of know what a mobile app can do, right? So many people, including non-technical people, have good instincts about what a mobile app can do. If you look at mature job roles, like sales, marketing, HR, legal, they're all really important, all really difficult. But there are enough marketers that have done marketing for long enough and the marketing tactics haven't changed that much in the last year.
Andrew Ng: So there are a lot of people that are really good in marketing. And it's really important, really hard, but that knowledge is relatively diffused because the knowledge of how to do HR, it hasn't changed dramatically in the last six months. But AI is emerging technology, and so the knowledge of how to do AI really well is not widespread. And so teams that actually get it, that understand AI, do have an advantage over teams that don't. Whereas if you have an HR problem, you can find someone that knows how to do it well, probably. But if an AI problem, knowing how to actually do that could put you ahead of other companies.
Andrew Ng: So things like, what accuracy can you get for customer service chatbot? You know, should your problem be fine-tuned using a genuine workflow? How do you get the voice out to low latency? There are a lot of these decisions that if you make the right technical decision, you can solve the problem in a couple of days. If they make the wrong technical decision, You could chase a blind alley for three months, right? And one thing I've been surprised by, it turns out if you have two possible architecture decisions, it's one bit of information. It feels like if you don't know the right answer, at most you're twice as slow, right?
Andrew Ng: One bit. Try both. It feels like one bit of information can at most buy you a 2X speed up. And I think in some theoretical sense, that is true. But what I see in practice, if you flip the wrong bit, you're not twice as slow. You spend like 10 times longer chasing a blind alley, which is why I think going into this right technical judgment, it really makes startups go so much faster. The other reason why I find staying on top of AI really hopeful for startups is over the last two years, we have just had a ton of wonderful Gen.AI tools or Gen.AI building blocks, right?
Andrew Ng: Partial list, but prompting, agented workflows, evals, guardrails, RAC, voice act, async programming, lots of ETL, embeddings, fine tuning, graph DB, how the computer use MCP using models. There's a long and wonderful list of building blocks. that you can quickly combine to build software that no one on the planet could have built even a year ago. And this creates a lot of new opportunities for startups to build new things. So when I learned about these building blocks, this is actually a picture that I have in mind. If you own one building block, like you have a basic white building block, you can build some cool stuff.
Andrew Ng: Maybe you know how to prompt, so you have one building block. You can build some amazing stuff. But if you get a second building block, like you also know how to build a chat box, so you have a white Lego break and a black Lego break, you can build something more interesting. If you acquire a blue building break as well, you can build something more interesting. Get a few red building breaks, maybe a little yellow one. more interesting, get more building breaks, get more building breaks, and very rapidly, the number of things you can combine them into grows kind of combinatorially or grows exponentially.
Andrew Ng: And so knowing all these wonderful building blocks lets you combine them in a much richer combination. One of the things that Deep Learning.AI does, so I actually take a lot of Deep Learning.AI short courses myself, to work with, I think, all the leading AI companies in the world, and try to hand out building blocks. But when I look at the Deep Learning.AI course catalog, this is actually what I see. And whenever I take these courses and learn these building blocks, I feel like I'm getting new things that can combine to form kind of combinatory or exponentially more software applications that were not possible just one or two years ago.
Andrew Ng: So just to wrap up, this is my last slide. Anyone take questions if y'all have any? I find that there are many things that matter for a startup, not just speed. But when I look at the startups that AI Fund is building, I find that the management team's ability to execute at speed is highly correlated with its odds of success. And some things we've learned to get you speed is work on concrete ideas. It's going to be good concrete ideas. I find that as an executive, I'm judged on the speed and quality of my decisions.
Andrew Ng: Both do matter, but speed absolutely matters. Rapid entering with AI coding assistance makes you go much faster, but that shifts the bottleneck to getting user feedback and to product decisions. And so having a portfolio of tactics to go get rapid feedback. And if you haven't learned to go to a coffee shop and talk to strangers, it's not easy, but then just be respectful, right? Just be respectful of people. That's actually a very valuable skill. entrepreneurs to have, I think. And I think also seeing on top of AI technology buys you speed. All right, with that, let me thank you very much.
Andrew Ng: Thank you. Have any quick questions?
SPEAKER_01: As AI advances, do you think it's more important for humans to develop the tools or learn how to use the tools better? How can we position ourselves to remain essential in a world where intelligence is becoming democratized?
Andrew Ng: I feel like AGI has been overhyped. And so for a long time, there'll be a lot of things that humans can do that AI cannot. And I think in the future, the people that are most powerful are the people that can make computers do exactly what you want it to do. And so I think staying on top of the tools, some of us will build tools sometimes, but there are a lot of other tools that others will build that we can just use. But so people that know how to use AI to get computers to do what you want it to do will be much more powerful.
Andrew Ng: Not worry about people running out of things to do, but people that can use AI will be much more powerful than people that don't.
SPEAKER_05: Hey, so well, first of all, thank you so much. I have a huge respect for you. And I think that you're true inspiration for a lot of us. My question is about the future of compute. So as we move towards more powerful AI, where do you think that compute is heading? I mean, we see people saying, let's ship GPUs to space. Some people talking about nuclear power data centers. What do you think about it?
Andrew Ng: There's something on debating what I wanted to say in response to the last question about kind of AGI, about maybe I'll answer this and a little bit of the last question. So it turns out there's one framework you can use for deciding what's hype and what's not hype. I think over the last two years, there's been a handful of companies that hyped up certain things for promotional, PR, fundraising, influence purposes. And because AI was so new, a handful of companies got away with saying almost anything without anyone fact checking them because the technology was not understood.
Andrew Ng: So one of my mental filters is there's certain hype narratives that make these businesses look more powerful that's been amplified. And so, for example, this idea that AI is so powerful, we might accidentally lead to human extinction. That's just ridiculous, but it is a hype narrative that made certain businesses look more powerful, and it got ramped up and actually helped certain businesses' fundraising goals. AI is so powerful, soon no one will even have a job anymore. Just not true, right? But again, that made these businesses look more powerful, got hyped up. Or, we are so powerful, So when the hype narrative was so powerful that by training a new model, we will casually wipe out thousands of startups.
Andrew Ng: That's just not true. Yes, Jasper ran into trouble, small number of companies got wiped out, but it's not that easy to casually wipe out thousands of startups. AI needs so much electricity, only nuclear power is good enough for that. That wind solar stuff, this is not true. So I think a lot of these GPUs in space, I don't know, Girlfriend, I think we have a lot of room to run still for terrestrial GPUs. Yeah, but I think some of these hype narratives have been amplified that I think are a distortion of what actually will be done.
SPEAKER_06: There's a lot of hype in AI and nobody's really certain about how we're going to be building the future with it. But what are some of the most dangerous biases or overhyped narratives that you've seen people talk about or get poisoned by that they end up running with that we should try to avoid or be more aware of and allow us to have a more realistic view as we are building this future?
Andrew Ng: So I think the dangerous AI narrative has been overhyped. AI is a fantastic tool, but like any other powerful tool, like electricity, lots of ways to use it for beneficial purposes, also some ways to use it in harmful ways. I find myself not using the term AI safety that much, not because I think we should build dangerous things, but because I think safety is not a function of technology, it's a function of how we apply it. So like electric motor. The maker of electric motor can't guarantee that no one will ever use it from unsafe downstream tiles.
Andrew Ng: Electric motor can be used to build a dialysis machine, electric vehicle, can be used to build a smart bomb, but the electric motor manufacturer can't control how we use downstream. So safety is not a function of the electric motor, it's a function of how you apply it. And I think the same thing for AI. AI is neither safe nor unsafe. It is how you apply it that makes it safe or unsafe. So instead of thinking about AI safety, I often think about responsible AI, because it is how we use it responsibly, hopefully, or irresponsibly that determines whether or not what we build with AI technology ends up being harmful or beneficial.
Andrew Ng: And I feel like sometimes that the really weird corner cases, they get high top of the news. I think just one or two days ago, there was a Wall Street Journal article about AI losing control of AI or something. And I feel like that article took important case experiments run in a lab and sensationalize it in a way that I think was really disproportionate relative to the lab experiment that was being run. And unfortunately, technology is hard enough to understand that many people don't know better. And so these hype narratives do keep on getting amplified.
Andrew Ng: And I feel like this has been used as a weapon against open source software as well, which is really unfortunate.
SPEAKER_03: Thank you for your work. I think your impact is remarkable. My question is, as aspiring founders, how should we be thinking about business in the world where anything can be disrupted in a day? Whatever great mode product or feature you have can be replicated with VibeCoding competitors in basically hours.
Andrew Ng: It turns out when you start a business, there are a lot of things to worry about. The number thing I worry about is, are you building a product that users love? It turns out that when you build a business, there are lots of things to think about. There goes market channel, competitors, technology mode, all that is important. But if I were to have a singular focus on one thing, it is, are you building a product that users really want? Until you solve that, it's very difficult to build a valuable business. After you solve that, the other questions do come to play.
Andrew Ng: Do you have a channel to get to customers? What is pricing long-term? What is your moat? I find that moats tend to be over-hyped, actually. I find that more businesses tend to start off with a product and then evolve eventually into a moat. But consumer products brand is somewhat more defensible. And if you have a lot of momentum, it becomes harder to catch you. But enterprise products, sometimes if you have a maybe moat is more of a consideration of their channels that are hard to get into enterprises. So I think Sorry, when AI Fund looks at businesses, we actually wind up doing a fairly complex analysis of these factors and writing to the six-page narrative memo to analyze it before we decide whether or not to proceed or not.
Andrew Ng: All of these things are important, but I feel like at this moment in time, the number of opportunities, meaning the amount of stuff that is possible that no one's built yet in the world seems much greater than the number of people with the skill to build them. So definitely at the application layer, it feels like there's a lot of white space for new things you can build that no one else seems to be working on. And I would say, you know, focus on building a product that people want, that people love. figure out the rest of it along the way, although that's important for you along the way.
SPEAKER_04: Hi, Professor. Thanks for your wonderful speech. I'm an undergrad researcher from Stanford, and I think your metaphor in your speech is very interesting. You said the current AI tools are like bricks and can be built upon accumulation. However, so far it is difficult to see the accumulative functional expansion of the integration of AI tools because they often align on the stacking of functions based on intent distribution and are accompanied by dynamic problems of tokens and time overhead. So which is different from static engineering? So what do you think will be the perspective of a possible Agent 2 accumulation effect in the future?
Andrew Ng: But hey, just some quick remarks to that, right? You mentioned agent LM token cost. My most common advice to developers is to first approximation, just don't worry about how much tokens cost. Only a small number of startups are lucky enough to have users use so much of your product that the cost of tokens becomes a problem. It can become a problem. I've definitely been on a bunch of teams where the cost, you know, users like our product, and we started to look at our, right, GNI bills, and it was definitely climbing in a way that really became a problem.
Andrew Ng: But it's actually really difficult to get to a point where your token usage caused our problem. And for the teams I'm on, where we were lucky enough that users made our token cause a problem, we often had entry solutions to then bend the cursor and bring it back down for prompting, fine-tuning, USDS bytes, optimizer, whatever. And then... What I'm seeing is that I'm seeing a lot of agentic workflows that actually integrate a lot of different steps. So, for example, if you build a customer service chatbot, we'll often have to use prompting, maybe optimize some of the results in DSPy, build evals, build guardrails.
Andrew Ng: Maybe the customer service chatbot needs to rack up all the way to get information to feed back to the user. So I actually do see these things grow. But one tip for many of you as well is I will often architect my software to make switching between different building block providers relatively easy. So for example, I have a lot of products that build on top of OMS, but sometimes you point to a specific product and ask me which OMS are we using. I honestly don't know because we've built up evals. And when there's a new model that's released, we'll quickly run evals to see if the new model is better than the old one.
Andrew Ng: And then, you know, just switch to the new model if the new model does better on evals. And so the model we use week by week, you know, sometimes our engines will change it without even bothering to tell me because evals show the new model works better. So it turns out that switching costs for foundation models is relatively low. And we often architect our software. Oh, AI Suite is open sourcing that my friends and I worked on to make switching easier. Switching costs for the orchestration platforms is a little bit harder. But I find that preserving that flexibility in your choice of building blocks often lets you go faster, even as you're building more and more things on top of each other.
SPEAKER_08: Thank you so much. In the world of education in AI, there are two paradigms mostly. So one is AI can make teachers more productive, automating grading and automating homeworks. But another school of thought is that there'll be personal tutors for every student. So every student can have one tutor that gets feedback from an AI and gets personal questions from them. So how do you see these two paradigms converge and how would education look like in the next five years?
Andrew Ng: I think everyone feels like a change is coming in at tech, but I don't think the disruption is here yet. I think a lot of people are experimenting with different things. So, you know, Coursera has Coursera Coach, which actually works really well. Deep learning AI is more focused on teaching AI, also has some built-in chat bars. A lot of teams are experimenting with auto grading. Oh, there's an avatar of me on the Deep Learning AI website you can talk to if you want. Deep learning AI slash avatar. And then I think for some things like language learning, with, you know, speak, Duolingo, that has become clearer some of the ways AI would transform it.
Andrew Ng: For the broader educational landscape, the exact ways that AI would transform it, I see a lot of experimentation. I think what key learning, which I've been doing some work with, is doing this is very promising for K-12 education. But I think what I'm seeing is, frankly, tons of experimentation, but the final end state is still not clear. I do think education will be hyper-personalized, but that workflow is an avatar, is a text chatbot. What's the workflow? I feel like the hype from a couple of years ago that what AGI is doing and it will be all so easy, that was hype.
Andrew Ng: The reality is, Work is complex, right? Teachers, students, people do really complex workflows. And for the next decade, we'll be looking at the work that needs to be done and figuring out how to map it to agented workflows. And education is one of the sectors where this mapping is still underway, but it's not yet mature enough to the point where the end state is clear. So I think we should all just keep working on it.
SPEAKER_08: All right. All right. Thank you so much, Andrew. Thank you.
SPEAKER_02: Hey, my question is, I think AI has a lot of great potential for good, but there's also a lot of potential for bad consequences as well, such as exacerbating economic inequality and things like that. And I think a lot of our startups here, while they'll be doing a lot of great things, will also be just by virtue of their product be contributing to some of those negative consequences. So I was curious, how do you think us as AI builders should balance our product building with also the potential societal downsides of some AI products? And essentially, how can we both move fast and be responsible, as you mentioned in your talk?
Andrew Ng: Look in your heart and if fundamentally what you're building, if you don't think it'll make people writ large better off, don't do it, right? I know it sounds simple, but that's really hard to do at the moment. But AI Fund, we've killed multiple projects, not on financial grounds, but on ethical grounds where there are multiple projects. We looked at The economic case is very solid, but we said, you know what, we don't want this to exist in the world, and we just killed it on that basis. So I hope more people will do that. And then I worry about bringing everyone with us.
Andrew Ng: One interesting thing I'm seeing is people in all sorts of job roles. that are not engineering are much more productive if they know AI than if they don't. And so, for example, on my marketing team, my marketers, they know how to code. Frankly, they're running circles around the ones that don't. So then everyone learned to code, and then they got better. But I feel like trying to bring everyone with us to make sure everyone is empowered to build with AI, that would be an important part of what all of us do, I think.
SPEAKER_07: I'm one of your big fans, and thank you for your online courses. Your courses make the diplomacy much more accessible to the world. And my question is also about education. As AI becomes more powerful and widespread, there seems to be a growing gap between what can actually do and what people perceive it. So what do you think about like, is it important to educate the general public about deep learning stuff and not only like educate those technical people and make people understand more what really what AI really do and how it works?
Andrew Ng: I think that knowledge will diffuse. Deep learning AI, we want to empower everyone to build with AI. So we're working on it. Many of us are working on it. I'll just tell you what I think is the main thing. I think there are maybe two dangers. One is if you don't bring people with us fast enough, I hope we'll solve that. There's one other danger, which is It turns out that if you look at the mobile ecosystem, mobile phones, it's actually not that interesting. And one of the reasons is there are two gatekeepers, Android and iOS.
Andrew Ng: And unless they let you do certain things, you're not allowed to try certain things on mobile. And I think this hampers innovators. These dangers of AI have been used by certain businesses. They're trying to shut down open source because a number of businesses love to be a gatekeeper to large scale foundation models. So I think piping up dangers, suppose false dangers of AI in order to get regulators to pass laws like the proposal of SB 1047 in California, which thank goodness we shut down, where they put in place really burdensome regulatory requirements that don't make anyone safer, but would make it really difficult for TS to release open source and open weight software.
Andrew Ng: So one of the dangers to inequality as well is if these regulatory, you know, awful regulatory approaches, and I've been in the room where some of these businesses said stuff to regulators that was just not true. So I think that some of these arguments, the danger is if these regulatory proposals succeed, and end up siphoning regulations, leaving us with a small number of gatekeepers, where everyone needs the permission of a small number of companies to fine-tune the model, prompt in a certain way. That's what will siphon innovation and prevent the diffusion of this information to let lots of startups, you know, build whatever they want responsibly, but are free to innovate.
Andrew Ng: So I think so long as we prevent this line of attack on open source open weight models from succeeding. And we've made good progress, but the threat is still there. Then I think eventually we'll get to the diffusion of knowledge and we can hopefully then bring everyone with us. But this fight to protect open source, we've been winning, but the fight is still on and we still have to keep up that work to protect open source. Thank you all very much. This is wonderful. Thank you.